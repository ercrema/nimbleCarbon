---
title: "Fitting and Comparing Growth Models with NimbleCarbon"
author: "Enrico Crema"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
vignette: >
  %\VignetteIndexEntry{Fitting and Comparing Growth Models with NimbleCarbon}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---


```{r general setup, include = FALSE}
h = 3.5
w = 3.5
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check)
```

# Introduction

The _nimbleCarbon_ package provides a suite of bespoke functions and statistical distribution for using NIMBLE models to fit and compare population growth models based on temporal frequencies of radiocarbon dates. [Nimble](https://cran.r-project.org/package=nimble) is an R package that provides a system for  for writing Bayesian statistical model using an extensible dialect of the BUGS model language and compiler that generates C++ programs for improved performance.  This document will provide a quick guide and users are strongly advised to visit Nimble's [website](https://r-nimble.org/) for further information.  

## Installing and loading the _nimbleCarbon_ package

The _nimbleCarbon_ package is still experimental and can be installed only via GitHub using the [devtools](https://cran.r-project.org/package=devtools) package:

```{r installing nimbleCarbon,eval=FALSE,message=FALSE,warning=FALSE}
library(devtools)
install_github('ercrema/nimbleCarbon',auth_token='58fd474f4001e364c30a642e5013972e5a2fb7ae')
```

Once the installation is completed the package can be loaded using the `library()` command:

```{r loading nimbleCarbon}
library(nimbleCarbon)
```

# Example 1: Exponential Growth Model

To illustrate a typical work-flow for fitting growth models with _nimbleCarbon_ we consider a hypothetical scenario where the target population grew with an exponential rate. More formally, we assume that the population size $N_t$ at time $t$ is given by the following equation:

$$N_t = N_0(1+r)^t$$

where $N_0$ is the initial population size at time $t=0$ and $r$ is the growth rate. The assumption of the so-called _dates as data_ approach (Rick 1987) is that the probability of $\pi_t$ of observing a $^14$C date at time $t$ is proportional to $N_t$. It follows that that given a temporal window consisting of $T$ years, $\pi_t$ can be described by the following equation:

$$ \pi_t = \frac{N_0(1+r)^t}{\sum_{t=1}^TN_0(1+r)^t}$$
and because $N_0$ is a constant and does not affect the estimate of $\pi_t$, we can further simplify the model by setting $N_0$ to 1. In order to take into account calibration effects we also need to define the specific calendar year of the index $t$. Thus the equation can be further as follows:

$$ \pi_{a-t} =  \frac{(1+r)^t}{\sum_{t=0}^{a-b}(1+r)^t}$$
where $a$ and $b$ are the calendar years defining the start and the end of the time window of analysis. Thus, for example, if we set $a=6000$, $b=4000$ and $r=0.001$ we can obtain a vector of probabilities as follows:

```{r exponential model,fig.width=5.5,fig.height=5}
a = 6500
b = 4500
r = 0.001
t = 0:(a-b)
pi = ((1+r)^t)/(sum((1+r)^t))
plot(a-t,pi,xlim=c(a,b),type='l',xlab='Cal BP',ylab='Probability Mass',ylim=c(0,max(pi)))
```

We can use the vector `pi` to generate sample calendar dates and then back-calibrate this to $^14$ C age. The example below is a hypothetical dataset consisting of 300 radiocarbon dates, each associated with an error of 20 years:

```{r generate samples from exponential model}
set.seed(123)
n = 300
calendar.dates = sample(a:b,size=n,prob=pi)
cra = round(uncalibrate(calendar.dates)$ccCRA) #back-calibrate in 14C ages
cra.error = rep(20,n) #assign error of 20 years
```

Typically, these calendar dates are calibrated and their probabilities aggregated to generate summed probability distributions (SPD). Estimates of growth rates are then obtained by fitting a regression model where the response variable is the vector of summed probabilities and the independent variable their corresponding calendar age. Such approach, however, does not take into account sampling error nor the impact of the calibration process. As a result estimates can be biased, they do not provide reliable measures of uncertainty, and the computed likelihood (and consequently derived measures such as AIC) are incorrect. Timpson et al (2021) have recently overcome this problem by using likelihood estimates of a given growth model as a generalized Bernoulli distribution where the probability vector for each discrete calendar year is derived by some some growth model such as the exponential model described above. This effectively makes the inferential process of population growth rates akin to those more commonly employed for the Bayesian analyses of archaeological phases (Buck et al 1992) in software packages such as [OxCal](https://c14.arch.ox.ac.uk/oxcal.html) and [BCal](https://bcal.shef.ac.uk/). The key difference is that in this context time is treated as discrete so that the likelihood function for any growth models can be regarded as a special case of a generalized Bernoulli distribution (also known as categorical distribution). We can thus define a bounded growth model a categorical distribution where the probabilities $\pi_{a-1},\pi_{a-2},...,\pi_{a-b+1}$ for each calendar year $a-t$ is defined by a function with a set a growth parameters and the boundary parameters $a$ and $b$ defining the start and the end of the window of analysis in cal BP.  

In order to carry out a Bayesian analysis of our simulated data using _Nimble_ we need to first define a BUGS model using the `nimbleCode()` function:

```{r exponential nimbleCode}
model <- nimbleCode({
      for (i in 1:N){
        # Growth Model Likelihood
        theta[i] ~ dExponentialGrowth(a=start,b=end,r=r);
        # Calibration
        mu[i] <- interpLin(z=theta[i], x=calBP[], y=C14BP[]);
        sigmaCurve[i] <- interpLin(z=theta[i], x=calBP[], y=C14err[]);
        sd[i] <- (sigma[i]^2+sigmaCurve[i]^2)^(1/2);
        X[i] ~ dnorm(mean=mu[i],sd=sd[i]);
      }
      # Prior
      r ~ dexp(1/0.004); # Prior
    })  
```

The syntax of the BUGS model typically include three main elements. The first one consist of a growth model (in this case `dExponentialGrowth()`) which defines the likelihood of observing a given calendar date (`theta`) within the time range of analysis defined by the parameters `a` and `b`, and in this case by the growth rate `r`. The second block effectively consists of calibrating `theta`, taking account for the Gaussian measurement error. For most applications this section can be copied and pasted as it is. Finally the third block defines the prior probability of our parameters --- in this case an exponential with a rate of 1/0.004, where 0.004 is the average growth rate observed in several prehistoric populations (see Zahid et al 2016). 

Next we define our constants and the data to be fitted. The constants would include the sample size, the values associated with the calibration curve, and any other fixed parameters (in this case `start` and `end`):

```{r constraints ex1}
data("intcal20") #load the IntCal20 calibration curve, Remier et al 2020
constants <- list(N=n,calBP=intcal20$CalBP,C14BP=intcal20$C14Age,C14err=intcal20$C14Age.sigma,start=a,end=b)
```

We then define our input data:

```{r data ex1}
data <- list(X=cra,sigma=cra.error)
```

We are now ready to compile and run our model. The nimble package offer different options and degrees of customisation. The quickest approach consist of using the `nimbleMCMC()` function, which requires various MCMC parameters such as the number of chains and iterations and some sensible initial values. Initial values can be fixed:

```{r ini ex1}
m.dates = medCal(calibrate(cra,cra.error,verbose = FALSE))
if(any(m.dates>a|m.dates<b)){m.dates[m.dates>a]=a;m.dates[m.dates<b]=b} #ensure that theta is within the time range of analysis
inits <- list(r=0.0004,theta=m.dates)
```

or alternatively when running multiple chains can be defined as series of functions or a mixture of functions and fixed values:

```{r ini function ex1}
inits.function = function() list(r=rexp(1,1/0.0004),theta=m.dates)
```

The example below consists of 10,000 iterations over 2 chains with a burn-in of 2000 steps^[This took approximately 20 minutes on a i7-7560U linux machine with 16G of RAM]:

```{r mcmc samples ex1,results='hide',message=FALSE,warning=FALSE}
mcmc.samples<- nimbleMCMC(code = model,constants = constants,data = data,niter = 10000, nchains = 2, thin=1, nburnin = 2000, progressBar = FALSE, monitors=c('r','theta'), inits=inits.function, samplesAsCodaMCMC=TRUE,setSeed=c(123,456))
```

## Diagnostics and Posterior Distributions

The argument `samplesAsCodaMCMC` in `nimbleMCMC()` ensures that the output is stored as a `mcmc` class object of the [coda](https://cran.r-project.org/package=coda) package, which offers a wide range of MCMC diagnostics. First, trace plots of the posterior samples can be plotted using the standard `plot()` function in R as shown in the example below:

```{r trace plot ex1, fig.height=6.5,fig.width=5.5}
par(mfrow=c(2,1))
plot(as.numeric(mcmc.samples$chain1[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='chain 1')
plot(as.numeric(mcmc.samples$chain2[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='chain 2')
```

If more than two chains are available, diagnostic metrics such as Gelman-Rubin convergence diagnostic ($\hat{R}$, Gelman and Rubin 1992) and MCMC effective sample sizes can be evaluated:

```{r diagnostic ex1}
library(coda)
rhat = gelman.diag(mcmc.samples)
head(rhat$psrf)
ess = effectiveSize(mcmc.samples)
head(ess)
```

In this case the key parameter $r$ has an $\hat{R}$ between `r round(rhat$psrf[1,1],3)` and `r round(rhat$psrf[1,2],3)`, with an effective sample size of `r round(ess[1])` indicating a good convergence. The `postHPDplot()` in the _nimbleCarbon_ package offers a convenient way to display marginal posterior distributions highlighting user-defined highest posterior density interval:

```{r posterior ex1, fig.height=5,fig.width=5}
postHPDplot(mcmc.samples$chain1[,'r'],rnd=5,xlab='r',ylab='Density',prob = 0.95)
```

While the marginal posterior can provide direct insights on each parameter uncertainty, models with more than one parameters can be hard to interpret. The `modelPlot()` visualises the shape of growth models given a list of parameter combinations This can be used to carry out prior predictive checks (see left panel below) or to plot the fitted growth model incorporating the uncertainty of the parameters (right panel below).  

```{r, prior and posterior plot ex1, fig.width=7,fig.height=4}
par(mfrow=c(1,2))
set.seed(123)
modelPlot(dExponentialGrowth,a=a,b=b,params=list(r=rexp(100,1/0.0004)),alpha = 0.1,ylim=c(0,0.003),main='Prior',type='spaghetti')
lines(a:b,pi,col=2,lty=2,lwd=2)
modelPlot(dExponentialGrowth,a=a,b=b,params=list(r=mcmc.samples$chain1[,'r']),nsample=100,alpha=0.1,ylim=c(0,0.003),main='Posterior',type='spaghetti')
lines(a:b,pi,col=2,lty=2,lwd=2)
```

# Example 2: Logistic Growth and Model Comparison

## Data Preparation

Now let's consider another growth model by examining an empirical case study. We will use a subset of the EUROEVOL dataset (Manning et al 2016) provided in the _rcarbon_ package (Crema and Bevan 2020), examining the radiocarbon record from Denmark:

```{r ex2 data prep 1}
data(euroevol)
DK=subset(euroevol,Country=="Denmark") #subset of Danish dates
DK.caldates=calibrate(x=DK$C14Age,errors=DK$C14SD,calCurves='intcal20',verbose=FALSE) #calibration
```

In the case of SPD analysis a common practice for handling inter-site variation in sampling intensity consists of grouping dates that are close in time, aggregating their summed probabilities and normalise to sum to unity (see Timpson et al 2014, Crema and Bevan 2020 for details). This approach is not possible here, where we instead select a random date from each 'bin'. This can be achieved using the `binPrep()` ad `thinDates()` functions in _rcarbon_:

```{r ex2 binning and thinning}
# Generate bins grouping dates within 100 yrs
DK.bins = binPrep(sites=DK$SiteID,ages=DK$C14Age,h=100) 
# Sample 1 date from each bin, selecting randomly the sample with the smallest error
DK.caldates = DK.caldates[thinDates(ages=DK$C14Age,  errors=DK$C14SD, bins=DK.bins, size=1, thresh=1,seed=123,method='splitsample')]
```

We will in this case examine the density of radiocarbon dates between 8000 and 4500 cal BP. To obtain the relevant subset from `DK.caldates` we will use the `subset()` function, taking into consideration only samples with a cumulative probability equal or larger than 0.5 within the time window of analysis:

```{r ex2 subsetting}
DK.caldates = subset(DK.caldates,BP<=8000&BP>=4500,p=0.5)
```

Finally we extract the $^{14}$C ages and the associated errors from our subset

```{r ex2 extracting cra and errors}
obs.CRA = DK.caldates$metadata$CRA
obs.Errors = DK.caldates$metadata$Error
```

and visualise the SPD distribution of the resulting subset using the `spd()` function in _rcarbon_

```{r ex2 spd,fig.width=6,fig.height=5}
obs.spd = spd(DK.caldates,timeRange=c(8000,4500),verbose=FALSE)
plot(obs.spd)
```

Finally, before defining our growth models we can set our input data and constants as we did earlier:

```{r ex2 constants and data}
constants <- list(N=length(obs.CRA),calBP=intcal20$CalBP,C14BP=intcal20$C14Age,C14err=intcal20$C14Age.sigma,start=8000,end=4500)
data <- list(X=obs.CRA,sigma=obs.Errors)
```

## Growth Models

We will consider an exponential and a logistic growth model. The former is the same as what we used earlier:

```{r ex2 exponential model}
m1 <- nimbleCode({
      for (i in 1:N){
        # Growth Model Likelihood
        theta[i] ~ dExponentialGrowth(a=start,b=end,r=r);
        # Calibration
        mu[i] <- interpLin(z=theta[i], x=calBP[], y=C14BP[]);
        sigmaCurve[i] <- interpLin(z=theta[i], x=calBP[], y=C14err[]);
        sd[i] <- (sigma[i]^2+sigmaCurve[i]^2)^(1/2);
        X[i] ~ dnorm(mean=mu[i],sd=sd[i]);
      }
      # Prior
      r ~ dexp(1/0.004); # Prior
    })  

```

The logistic growth model in _nimbleCarbon_ is defined as follows:

$$ \pi_{a-t} =  \frac{\frac{1}{1+\frac{1-k}{k}e^{-rt}}}   {\sum_{t=0}^{a-b}\frac{1}{1+\frac{1-k}{k}e^{-rt}}}$$

where $k$ is size of the population at $t=0$, expressed as the proportion of the carrying capacity. The numerator of the right term is a special case of the following equation:

$$ N_{t} = \frac{K}{1+\frac{K-N_0}{N_0}e^{-rt}} $$

where the carrying capacity $K$ is set to 1, and $N_0=k$. To make a `nimbleCode` of the logistic growth model we will use `dLogisticGrowth` which requires the boundary parameters `a` and `b`, the initial population size `k`, and the intrinsic growth rate `r`. Here we fix `a` and `b` to the boundary of our time-window, while and use an exponential prior for `k` and `r`. Notice that for the former we truncate between 0.001 and 0.2 using the `T()` syntax in _nimble_: 

```{r ex logistic model}
m2 <- nimbleCode({
      for (i in 1:N){
        # Growth Model Likelihood
        theta[i] ~ dLogisticGrowth(a=start,b=end,k=k,r=r);
        # Calibration
        mu[i] <- interpLin(z=theta[i], x=calBP[], y=C14BP[]);
        sigmaCurve[i] <- interpLin(z=theta[i], x=calBP[], y=C14err[]);
        sd[i] <- (sigma[i]^2+sigmaCurve[i]^2)^(1/2);
        X[i] ~ dnorm(mean=mu[i],sd=sd[i]);
      }
      # Prior
      r ~ dexp(1/0.004); # Prior
      k ~ T(dexp(1/0.05),0.001,0.2)
    })  
```

We are now ready to define our initialisation functions for the two models so that chains have different starting parameter values for the growth models. 

```{r ex2 init}
m.dates = medCal(DK.caldates)
if(any(m.dates>a|m.dates<b)){m.dates[m.dates>a]=a;m.dates[m.dates<b]=b}
inits.function.m1 = function() list(r=rexp(1,1/0.0004),theta=m.dates)
inits.function.m2 = function() list(r=rexp(1,1/0.0004),k=runif(1,0.0001,0.2),theta=m.dates)
```

## MCMC, Diagnostic, Model Comparison, and Posterior Predictive Check

We can now use the `nimbleMCMC()` function again, but this time we: 1) define the random seed using the `setSeed` argument to ensure full reproducibility; and 2) we set the argument `WAIC` to TRUE so the models can be compared using the Widely Applicable Information Criterion (Watanabe 2010, Gelman et al 2014) : 

```{r ex2 mcmc,message=FALSE,warning=FALSE}
mcmc.samples.m1<- nimbleMCMC(code = m1,constants = constants,data = data,niter = 15000, nchains = 2, thin=1, nburnin = 3000, progressBar = FALSE, monitors=c('r','theta'), inits=inits.function.m1, samplesAsCodaMCMC=TRUE,setSeed=c(123,456),WAIC=TRUE)
mcmc.samples.m2<- nimbleMCMC(code = m2,constants = constants,data = data,niter = 15000, nchains = 2, thin=1, nburnin = 3000, progressBar = FALSE, monitors=c('r','k','theta'), inits=inits.function.m2, samplesAsCodaMCMC=TRUE,setSeed=c(123,456),WAIC=TRUE)
```

Model diagnostics and the trace plot indicates a fairly good convergence for both models

```{r ex2 diagnostic, fig.width=6.5,fig.height=7}
par(mfrow=c(3,2))
plot(as.numeric(mcmc.samples.m1$samples$chain1[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='m1 r chain 1')
plot(as.numeric(mcmc.samples.m1$samples$chain2[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='m1 r chain 2')
plot(as.numeric(mcmc.samples.m2$samples$chain1[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='m2 r chain 1')
plot(as.numeric(mcmc.samples.m2$samples$chain2[,'r']),type='l',xlab='MCMC Iteration',ylab='r',main='m2 r chain 2')
plot(as.numeric(mcmc.samples.m2$samples$chain1[,'k']),type='l',xlab='MCMC Iteration',ylab='r',main='m2 k chain 1')
plot(as.numeric(mcmc.samples.m2$samples$chain2[,'k']),type='l',xlab='MCMC Iteration',ylab='r',main='m2 k chain 2')

m1.rhat=gelman.diag(mcmc.samples.m1$samples)
m2.rhat=gelman.diag(mcmc.samples.m2$samples)
m1.ess=effectiveSize(mcmc.samples.m1$samples)
m2.ess=effectiveSize(mcmc.samples.m2$samples)
head(m1.rhat$psrf)
head(m2.rhat$psrf)
m1.ess[1]
m2.ess[1:2]
```

indicating that we can have reliable marginal posterior distributions of our two models 

```{r ex2 marginal posteriors,fig.width=9,fig.height=3.5}
par(mfrow=c(1,3))
postHPDplot(mcmc.samples.m1$samples$chain1[,'r'],rnd=5,xlab='r',ylab='Density',prob = 0.95,main='Model 1: r',xlim=c(0.00055,0.0032))
postHPDplot(mcmc.samples.m2$samples$chain1[,'r'],rnd=5,xlab='r',ylab='Density',prob = 0.95,main='Model 2: r',xlim=c(0.00055,0.0032))
postHPDplot(mcmc.samples.m2$samples$chain1[,'k'],rnd=5,xlab='k',ylab='Density',prob = 0.95,main='Model 2: k')
```

as well as their shapes

```{r posterior model plot ex2, fig.width=7,fig.height=4}
params.m1 = list(r=c(mcmc.samples.m1$samples$chain1[,'r'],mcmc.samples.m1$samples$chain2[,'r']))
params.m2 = list(r=c(mcmc.samples.m2$samples$chain1[,'r'],mcmc.samples.m2$samples$chain2[,'r']),k=c(mcmc.samples.m2$samples$chain1[,'k'],mcmc.samples.m2$samples$chain2[,'k']))

par(mfrow=c(1,2))
set.seed(123)
modelPlot(dExponentialGrowth,a=8000,b=4500,params=params.m1,nsample=100,alpha = 0.1,ylim=c(0,0.001),main='m1: Exponential',type='envelope')
modelPlot(dLogisticGrowth,a=8000,b=4500,params=params.m2,nsample=100,alpha = 0.1,ylim=c(0,0.001),main='m2: Logistic',type='envelope')
```

The shape of the fitted model have some differences so it is worth asking which model provides a better fit to the observed data. We can formally evaluate this question by comparing the WAIC values obtained from the `nimbleMCMC()` function. The _nimbleCarbon_ package provide a handy function for extracting these and computing $\Delta WAIC$ and WAIC weights:

```{r model comparison ex2}
compare.models(mcmc.samples.m1,mcmc.samples.m2)
```

The results indicate a strong support for model 2 over model 1 in this case. While this provides a relative measure of the model's predictive performance we cannot tell whether it can comprehensively explain the observed temporal frequencies of radiocarbon dates. One way to evaluate the model performance in absolute terms is to generate SPDs from the fitted posterior models and visually compare this to the observed SPD. The approach is similar to Monte-Carlo Null-Hypothesis Testing approach introduced by Shennan et al (2013) and implemented in _rcarbon_'s `modelTest()` function, and it is effectively a form of posterior predictive check. The `postPredSPD()` function in _nimbleCarbon_ can be used to generate observed and an ensemble of fitted SPDs using the posterior samples obtained from `nimbleMCMC()`:

```{r posterior predictive check ex2,fig.width=6.5,fig.height=5}
set.seed(123)
pp.check.m2=postPredSPD(obs.CRA,errors = obs.Errors,calCurve = 'intcal20',model = dLogisticGrowth,a = 8000,b=4500,params=list(r=mcmc.samples.m2$samples$chain1[,'r'],k=mcmc.samples.m2$samples$chain1[,'k']),nsim = 100,ncores = 3,verbose=FALSE)
plot(pp.check.m2)
```

In this case, although the model (grey envelope) shows generally a good fit to the data, there are some intervals where the observed density of radiocarbon dates is lower (show in blue) or higher (shown in red) than the fitted model. 

# Example 3: Phase Models

The _nimble_ framework can also be used for phase modelling where the objective is to identify boundary estimates for a group of dates. The example below illustrates this with a simulated dataset with a start date at 4500 cal BP and an end date at 3800 cal BP:

```{r phase model,fig.width=7.5,fig.height=4.5,message=FALSE,warning=FALSE}
# Simulate Observed Data
a = 4500
b = 3800
set.seed(123)
n = 300
calendar.dates = sample(a:b,size=n)
cra = round(uncalibrate(calendar.dates)$ccCRA) #back-calibrate in 14C ages
cra.error = rep(20,n) #assign error of 20 years

# Define NIMBLE Model
phasemodel <- nimbleCode({
  for (i in 1:N){
    #  Likelihood
    theta[i] ~ dunif(alpha[1],alpha[2]);
    # Calibration
    mu[i] <- interpLin(z=theta[i], x=calBP[], y=C14BP[]);
    sigmaCurve[i] <- interpLin(z=theta[i], x=calBP[], y=C14err[]);
    sd[i] <- (sigma[i]^2+sigmaCurve[i]^2)^(1/2);
    X[i] ~ dnorm(mean=mu[i],sd=sd[i]);
  }
  # Prior
  alpha[1] ~ dunif(0,50000);
  alpha[2] ~ T(dunif(0,50000),alpha[1],50000)
})  

#define constant, data, and inits:
data("intcal20") 
constants <- list(N=n,calBP=intcal20$CalBP,C14BP=intcal20$C14Age,C14err=intcal20$C14Age.sigma)
data <- list(X=cra,sigma=cra.error)
m.dates = medCal(calibrate(cra,cra.error,verbose = FALSE))
inits <- list(alpha=c(3000,5000),theta=m.dates)

#Run MCMC
mcmc.samples<- nimbleMCMC(code = phasemodel,constants = constants,data = data,niter = 20000, nchains = 1, thin=1, nburnin = 5000, progressBar = FALSE, monitors=c('alpha','theta'), inits=inits, samplesAsCodaMCMC=TRUE,set.seed(123))

#Plot Posteriors
par(mfrow=c(1,2))
postHPDplot(mcmc.samples[,'alpha[2]'],xlim=c(4600,4400),xlab='Cal BP',ylab='Posterior Probability',main='Start of Phase')
abline(v=a,lty=2)
postHPDplot(mcmc.samples[,'alpha[1]'],xlim=c(3900,3700),xlab='Cal BP',ylab='Posterior Probability',main='End of Phase')
abline(v=b,lty=2)
```

While the model is able to correctly recover the true parameter values, it should be noted that for general purposes dedicated software packages such as [BCal](https://bcal.shef.ac.uk/) and [OxCal](https://c14.arch.ox.ac.uk/oxcal.html) are recommended, as they provide automatic tuning of MCMC settings as well as  prior definitions and can they can easily encode more complex models with multiple phases and constraints with a better performance. However, users interested in utilising specific probability distributions or bespoke constraints that cannot be straightforwardly defined with these software packages might consider using this package. The _nimbleCarbon_ package does also offer a function (`agreementIndex()`) for computing agreement indices (Bronk-Ramsey 1995) to determine model consistency. The function requires the observed CRA values and the posterior samples of each date obtained by `nimbleMCMC()`: 

```{r}
theta = mcmc.samples[,-c(1:2)] #Exclude columns containing the posterior samples otherthan those associated with each date 
a=agreementIndex(cra,cra.error,calCurve='intcal20',theta=theta,verbose = F)
head(a$agreement) #individual agreement indices
a$overall.agreement #overall agreement index
```

Although estimates provided by _nimbleCarbon_ are comparable to those computed by _OxCal_ (see below) it is worth bearing in mind that estimates are computed from the posterior samples, and as such good convergence and a larger number of samples would provide more reliable estimates. 

```{r agreement index, fig.width=5.5,fig.height=5.5}
#the oxcAAR package provides an R interface for OxCal
library(oxcAAR) 
quickSetupOxcal()

# Oxcal Script
my_oxcal_code <- 'Plot()
 {
  Sequence()
  {
   Boundary("Start Phase X");
   Phase("Phase X")
   {
    R_Date("Date-001",4570,30);
    R_Date("Date-002",4455,35);
    R_Date("Date-003",4590,40);
    R_Date("Date-004",4540,40);
    R_Date("Date-005",4530,40);
    R_Date("Date-006",4595,26);
    R_Date("Date-007",4510,30);
    R_Date("Date-008",4557,25);
    R_Date("Date-009",4570,30);
    R_Date("Date-010",4580,50);
    R_Date("Date-011",4590,50);
    R_Date("Date-012",4560,40);
    R_Date("Date-013",4440,40);
    R_Date("Date-014",4470,40);
    R_Date("Date-015",4516,29);
    R_Date("Date-016",4522,27);
    R_Date("Date-017",4533,28);
    R_Date("Date-018",4590,30);
    R_Date("Date-019",4517,20);
   };
   Boundary("End Phase X");
  };
 };'

# Execute OxCal Model Locally and Recover Output
my_result_file <- executeOxcalScript(my_oxcal_code)
my_result_text <- readOxcalOutput(my_result_file)
# Extract vector of agreement indices
index=grep(".posterior.agreement",my_result_text)
tmp=my_result_text[index]
oxcal.aindex=unlist(lapply(strsplit(tmp,"[=,;]"),function(x){return(as.numeric(x[[2]]))}))


### Fit Phase Model using Nimble
cra = c(4570,4455,4590,4540,4530,4595,4510,4557,4570,4580,4590,4560,4440,4470,4516,4522,4533,4590,4517)
cra.error =c(30,35,40,40,40,26,30,25,30,50,50,40,40,40,29,27,28,30,20)
n = length(cra)  
m.dates = medCal(calibrate(cra,cra.error,verbose = FALSE))

data <- list(X=cra,sigma=cra.error)
constants <- list(N=n,calBP=intcal20$CalBP,C14BP=intcal20$C14Age,C14err=intcal20$C14Age.sigma)
inits <- list(alpha=c(5000,5500),theta=m.dates)

mcmc.samples<- nimbleMCMC(code = phasemodel,constants = constants,data = data,niter = 100000, nchains = 1, thin=1, nburnin = 10000, progressBar = FALSE, monitors=c('alpha','theta'), inits=inits, samplesAsCodaMCMC=TRUE,setSeed = c(12345))

# Compute Agreement Index
a.nimble=agreementIndex(cra,cra.error,theta=mcmc.samples[,-c(1:2)],verbose=FALSE)

# Compare Agreement Indices
plot(oxcal.aindex,a.nimble$agreement,pch=20,xlab='OxCal Agreement Index',ylab='nimbleCarbon Agreement Index')
abline(a=0,b=1,lty=2)
```

# References

Bronk-Ramsey, C. (1995). Radiocarbon Calibration and Analysis of Stratigraphy: The OxCal Program. Radiocarbon, 37(2), 425–430. https://doi.org/10.1017/S0033822200030903

Buck, C. E., Litton, C. D., & Smith, A. F. M. (1992). Calibration of radiocarbon results pertaining to related archaeological events. Journal of Archaeological Science, 19(5), 497–512. https://doi.org/10.1016/0305-4403(92)90025-X

Crema, E. R., & Bevan, A. (2020). Inference from large sets of radiocarbon dates: software and methods. Radiocarbon, 1–17. https://doi.org/10.1017/RDC.2020.95

Gelman, A., & Rubin, D. B. (1992). Inference from Iterative Simulation Using Multiple Sequences. Statistical Science, 7(4), 457–472. https://www.jstor.org/stable/2246093. Accessed 12 January 2021

Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2

Manning, K., Colledge, S., Crema, E., Shennan, S., & Timpson, A. (2016). The Cultural Evolution of Neolithic Europe. EUROEVOL Dataset 1: Sites, Phases and Radiocarbon Data. Journal of Open Archaeology Data, 5(0). https://doi.org/10.5334/joad.40

Reimer, P. J., Austin, W. E. N., Bard, E., Bayliss, A., Blackwell, P. G., Ramsey, C. B., et al. (2020). The IntCal20 Northern Hemisphere Radiocarbon Age Calibration Curve (0–55 cal kBP). Radiocarbon, 62(4), 725–757. https://doi.org/10.1017/RDC.2020.41

Rick, J. W. (1987). Dates as Data: An Examination of the Peruvian Preceramic Radiocarbon Record. American Antiquity, 52(1), 55. https://doi.org/10.2307/281060

Shennan, S., Downey, S. S., Timpson, A., Edinborough, K., Colledge, S., Kerig, T., et al. (2013). Regional population collapse followed initial agriculture booms in mid-Holocene Europe. Nature Communications, 4, ncomms3486. https://doi.org/10.1038/ncomms3486

Timpson, A., Barberena, R., Thomas, M. G., Méndez, C., & Manning, K. (2021). Directly modelling population dynamics in the South American Arid Diagonal using 14C dates. Philosophical Transactions of the Royal Society B: Biological Sciences, 376(1816), 20190723. https://doi.org/10.1098/rstb.2019.0723

Timpson, A., Colledge, S., Crema, E., Edinborough, K., Kerig, T., Manning, K., et al. (2014). Reconstructing regional population fluctuations in the European Neolithic using radiocarbon dates: a new case-study using an improved method. Journal of Archaeological Science, 52, 549–557. https://doi.org/10.1016/j.jas.2014.08.011

Watanabe, S. (2010). Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. The Journal of Machine Learning Research, 11, 3571–3594.

Zahid, H. J., Robinson, E., & Kelly, R. L. (2016). Agriculture, population growth, and statistical analysis of the radiocarbon record. Proceedings of the National Academy of Sciences, 113(4), 931–935. https://doi.org/10.1073/pnas.1517650112
